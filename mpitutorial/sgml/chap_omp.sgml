<chapter>
<title>Using OpenMP</title>

<para>With the advance of relatively cheap multi-core CPU's it has
become much more attractive to use OpenMP. In contrast to MPI it will
only work on SMP boxes and not on clusters of networked computing nodes,
but at first sight OpenMP's programming model seems so much simpler
than MPI's that it seems worth a try. As Oscar Wilde said: &quot;I can resist
anything - but temptation.&quot;.</para>

<para>My first try to use OpenMP looked similar to
<computeroutput>apple_omp.1.c</computeroutput> (<xref
linkend="apple.omp.1.c"/>). Unfortunately this does not work too good:
</para>

<screen>
> time ./apple_omp.1
...
real    0m4.078s
user    0m5.424s
sys     0m0.016s
</screen>

    <figure id="FIG.ITAC.APPLE.OMP.1.P4.FULLSPAN">
    <title>OpenMP with initial loadbalancing problems.</title>
        <mediaobject> <imageobject>
        <imagedata align="center" 
                   fileref="../graphics/itac_apple_omp_1_p4_fullspan.png" 
                   format="PNG" scale="66"/>
    </imageobject></mediaobject></figure>


<para>This program ran on 4 cores using 4 threads and was not much better than
the serial version. This is a little disappointing. The reason is that in 
apple_omp.1 a very dumb static scheduling is used. The range of rows is
just cut into four areas so that one thread gets all the expensive
rows that belong to the Mandelbrodt set. Refer to <xref linkend="FIG.ITAC.APPLE.OMP.1.P4.FULLSPAN"/>: the
computing function iterate is colored while everything else is gray. The Load
Balance tab of the function profile shows that one thread does the lions share
of the computation.</para>

<para>This tracefile for this non-MPI program was generated by using the 
trace collectors API and then linking against a null implementation (see the 
Makefile). Running this binary yields next to no overhead but no tracefile 
either. Using binary instrumentation (man itcinstrument) a fully instrumented 
binary was produced.</para>

<para>Adding a single directive <userinput>schedule(dynamic, 1)</userinput>
 to the OpenMP pragma results in <computeroutput>apple_omp.2.c</computeroutput> (<xref linkend="apple.omp.2.c" />) and solves the problem :</para>

<screen>
> time ./apple_omp.2
...
real    0m1.263s
user    0m4.916s
sys     0m0.016s
</screen>

<para>Now that is fast! For this particular program and on this small
SMP box OpenMP seems to be the tool of choice. The advantage of OpenMP
is that it seems so simple to parallelize an application without doing
any restructuring of the code base. But after harvesting some low
hanging fruits you might end up with the conclusion that further
performance improvements can only be done with a restructuring of the
code.</para>

<para>For those programers who still are hoping for a free lunch when
parallelizing applications and porting them to clusters there is still 
hope: Cluster OmpenMP or clomp for short (sounds like the wooden shoes 
of the Dutch). This is available from Intel as an additional product
on top of their C++ and Fortran compilers for x86&#95;64-Linux.</para>

<para>Despite the fact that clomp is a lot more lightweight than
creating a single system image illusion in a cluster it can fail to
deliver acceptable performance when an application shares to much data
for read and write in a parallel region. As the rumours have it, there
are some OpenMP codes that scale surprisingly well with clomp. I never
saw that myself, but I guess apple_omp.2 would be a good candidate.</para>

</chapter>
